

```
-- Andrew Ng 교수님의 Neural Networks and Deep Learning 강의의 1주차 강의를 듣고 필기한 내용입니다.
-- 강의는 코세라와 Edwith에서 들었으며, 코딩 과제 자료는 코세라에서 보고 따라갔습니다.
```

## 4. Deep Neural Network

### 4.1. Deep L-layer Neural Network

- logistic regression:  은닉층이 없는 얕은 신경망
- 얼마나 깊게 만들어야 할지 미리 아는 것은 쉽지 않음. 다양한 값을 시도하고 평가해야 함
- 4층 신경망의 경우,
  - $l = 4$ (층의 수)
  - $n^{[l]}$ = # units in layer $l$
    - $n^{[1]}$ : 1층 레이어의 유닛 수
    - $n^{[4]} = n^{[L]}$ : 4층(마지막층) 레이어의 유닛 수
  - $a^{[l]}$ = 활성값(activations) in layer $l$
    -  $a^{[0]}$ : 입력 특징 ($X$)
    -  $a^{[L]}$ : 예측된 출력값 ( $\hat y$)
  - $w^{[l]}$ = $z^{[l]}$의 가중치

### 4.2. **Forward and Backward Propagation**

- 정방향 전파
  - 입력값 : $a^{[l-1]}$
  - 출력값 : $a^{[l]}$ , cache( $z^{[l]}$)
    - $w^{[l]}a^{[l-1]} + b^{[l]}$ 을 계산하여 $z^{[l]}$을 구함
    - $g^{[l]}(z^{[l]})$을 계산하여 $a^{[l]}$ 을 구함. 여기서 $g()$는 활성화 함수로, 각 층마다 다르게 설정(예: 1번 층에선 렐루 함수, 마지막 층에선 시그모이드 함수로 설정)

- 역방향 전파
  - 입력값 : $da^{[l]}$
  - 출력값 : $da^{[l-1]}$, $dW[l]$, $db^{[l]}$
    - $da^{[l]} * g^{[l]}(z^{[l]})$을 계산하여 $dz^{[l]}$을 구함. 이때 $z^{[l]}$는 정방향 전파시 캐시해두었던 값을 사용.
    - $dz^{[l]}a^{[l-1]}$을 계산하여 $dW^{[l]}$을 구함
    - $db^{[l]} = dz^{[l]}$ 이므로, $da^{[l-1]} = W^{[l]T}dz^{[l]}$
    - $dz^{[l]} = W^{[l+1]T}dz^{[l+1]}*g^{[l]'}(z^{[l]})$

![스크린샷 2019-12-04 오전 12.57.07](/Users/apple/Desktop/스크린샷 2019-12-04 오전 12.57.07.png)

### 4.3. **Forward Propagation in a Deep Network**

- 정방향 연산을 벡터화하면
  - $Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$
  - $A^{[1]} = g^{[1]}(Z^{[1]})$
  - ... 반복 
  - $\hat y = g(Z^{[4]}) = A^{[4]}$
- 행렬 차원에서 생각해버릇해야 오류 디버깅이 쉬움



### 4.4. **Getting Matrix Dimensions Right**

- 5층 신경망이 있다고 해보자
  - $n^{[1]}=3, n^{[2]}=5, n^{[3]}=4, n^{[4]}=2, n^{[5]}=1$ 의 형태이고, 입력값 $x$는 2개이가 있다.
  - $z^{[1]} = W^{[1]}x + b^{[1]}$를 구성하는 각 값의 형태는? 
    - $Z^{[1]}$는 1층 신경망의 유닛 수가 3개 이므로 $(3,1)$ 형태
    - $A^{[0]} = X$ 는 2개 이므로  $(2,1)$ 형태
    - `Z.shape` = `W.shape` * `x.shape`이어야 함. 
    - 그럼, $W.shape * (2,1) = (3,1)$ 이므로 $W$의 형태는 $(3,2)$여야 함
  - 일반화하면, 
    -  $W^{[l]} : (n^{[l]}, n^{[l-1]})$이고, $b^{[l]} : (n^{[l]}, 1)$
    -  $dW^{[l]} : (n^{[l]}, n^{[l-1]})$ 이고, $db^{[l]} : (n^{[l]}, 1)$
    -  $z^{[l]}, a^{[l]}$: $(n^{[1]}, 1)$이다. $z^{[l]} = g^{[l]}(a^{[l]})$이므로, $a^{[l]}$과 $z^{[l]}$은 같은 차원이어야 함
    -  $Z^{[l]}, A^{[l]}:(n^{[1]}, m)$이다. 
    -  $dZ^{[l]}, dA^{[l]}:(n^{[1]}, m)$이다. 

### 4.5. **Why Deep Representations?** 

- 왜 깊은 신경망이 잘 작동할까? 
- 직관1
  - 앞쪽 층 --> 뒷쪽 층으로 갈수록 간단한 특징 찾기 --> 특징 모아 복잡한 특징 찾기 반복
  - 이미지에서는 모서리로 보이는 위치 찾고> 조합해서 얼굴 윤곽 찾고 -> 눈코입 찾는 식
  - 소리 데이터에서는, 음소 찾고,  단어 찾고, 문장 찾기
- 직관2
  - 얕은 네트워크로 같은 함수를 계산하려고 하면, 기하급수적으로 많은 은닉 유닛이 계산에 필요함



### 4.7. **Building Blocks of a Deep Neural Network** 

==> 4.2 내용 반복..



### 4.8 **Parameters vs Hyperparameters**

- Parameters : 
  - ​	$W$, $b$ 

- Hyperparameters: 
  - 파라미터 $W$, $b$를 통제하는 변수. 
  - 학습률(learning rate,  $\alpha$)
  - 반복횟수(numbers of iteration)
  - 은닉층의 갯수(numbers of hidden layer, L)
  - 은닉유닛의 갯수(numbers of hidden units)
  - 활성화 함수의 선택(choice of activation function)
  - 모멘텀항(momentum term)
  - 미니배치 크기(mini batch size)