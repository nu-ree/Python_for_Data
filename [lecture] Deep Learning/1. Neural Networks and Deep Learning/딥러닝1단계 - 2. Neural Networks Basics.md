```
-- Andrew Ng 교수님의 Neural Networks and Deep Learning 강의의 1주차 강의를 듣고 필기한 내용입니다.
-- 강의는 코세라와 Edwith에서 들었으며, 코딩 과제 자료는 코세라에서 보고 따라갔습니다.
```

## 2. 신경망과 로지스틱회귀

### 2.1. Binary Classification

- 순전파와 역전파 개념을 이해하기 위해 로지스틱 회귀를 먼저 배워보자!
- 이진 분류(binary classification) 문제는 입력값을 받아 출력값을 2개 중 하나로 예측하여 반환하는 문제. 
- 가령 고양이이냐 아니냐, 스팸 메일이냐 아니냐 등의 문제가 이진 분류문제임



### 2.2. Logistic Regression

- 이진 분류 문제에서 예측 결과는 0과 1 사이의 확률값이어야 함
- 로지스틱 회귀에서는 시그모이드 함수를 이용해 예측 결과 값이 0과 1 사이의 값이 되게 변환함 
- $\hat{y} = \sigma({w^{T} + b})$
- $\sigma(z) = \dfrac{1} {(1+e^{-z})}$
- 시그모이드 함수는 $z$ 값이 매우 커지면 $e^{-z}$가 0에 수렴하여 $\sigma(z)$ 가 1에 가까워지고, $z$ 값이 매우 큰 음수가 되면  $e^{-z}$가 매우 큰 숫자가 되고,  $\sigma(z) = \dfrac{1}{1+ large number}$가 되어 0에 가까워짐



### 2.3. Logistic Regression Cost Function

#### Loss Function

- 손실함수($ L(y, \hat{y})$)란, 예측한 값($\hat{y}$)과 진짜 값($y$) 사이의 오차가 얼마나 큰지 측정하는 함수. 따라서 두 값의 차이가 크다면 손실함수$L$ 의 결과 값은 커져야 하고, 두 값의 차이가 작다면, 손실함수$L$ 의 결과 값은 작아져야 함
- 가장 흔하게 사용되는 비용함수는 오차제곱으로, 연속적인 값을 예측하는 회귀문제에서 손실함수를 $ L(\hat{y}, y)= \dfrac{1}{2}(\hat{y}- y)^2$ 로 정의함
- 하지만, 이진분류 문제는 최적화 함수가 볼록하지 않기 때문에(nonconvex), 이 비용함수를 그대로 적용하게 되면 여러개의 지역 최적값(local optima)을 가질 수 있음. 때문에 경사하강법(gradient descent)으로 전역 최적값(global optima)를 찾을 수 없음!
- 이진 분류에서는 아래와 같은 손실수를 사용함

>  $ L(y, \hat{y})=−(y \times log(\hat{y}) +(1−y)log(1−\hat{y}))$ 

- 실제 $y$ 값이 0 인 경우 $L(\hat{y}, y)=-log(1−\hat{y})$  가 됨. 예측값이 작아져야(0에 가까워져야) 손실함수 $ L$ 의 결과 값이 작아짐 
- 실제 $y$  값이 1인 경우  $L(\hat{y}, y)= -log(\hat{y})$  가 됨. 예측값이 커져야(1에 가까워져야) 손실함수 $ L$ 의 결과 값이 작아짐 

#### Cost Function

- 비용함수란, 손실함수를 각각의 훈련 샘플 적용한 값의 평균 값

- $J(w,b) = \dfrac{1}{m} \sum\limits_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)})$
- 매개면수 $w$와 $b$가 훈련세트 전체를 얼마나 잘 예측하는지 보여주는 함수

### 2.4. Gradient Descent

- 로직스틱 회귀에서 손실함수에 $ L(y, \hat{y})=−(y \times log(\hat{y}) +(1−y)log(1−\hat{y}))$ 를 사용하게 되면, 비용함수 $J$ 가 매개변수 $w$ 와 $b$ 에 대해 볼록한 형태를 가지게 됨
- 볼록한 형태가 되었으므로 **경사하강법**으로 쉽게 비용함수를 최소화 하는 $w$와 $b$를 찾을 수 있음. 경사하강법은 가장 가파른 방향으로 내려가 전역최적값을 찾음
- 경사하강법은 아래와 같음 $w = w-{\alpha} \times \frac{dJ(w)}{dw} = w-{학습률} \times {미분계수}$
  - $w$의 초기값을 큰 값으로 설정하게 되면, 미분 계수( $\frac{\partial J(w, b)}{\partial w}$)가 양수가 되어 $w$ 를 줄이는 방향으로 업데이트
  - $w$의 초기값을 작은 값으로 설정하게 되면, 미분 계수( $\frac{\partial J(w, b)}{\partial w}$)가 음수가 되어 $w$ 를 늘리는 방향으로 업데이트
  - 위와 같이 $w$를 업데이트 해나가면서 비용함수 $J(w)$를 최소화하는 지점을 찾음

- 참고로, 미분 계수( $\frac{\partial J(w,b)}{\partial w}$)는 $J(w)$ 가 $w$ 방향으로 얼마나 기울었는지를 의미함. 즉 함수의 기울기가 $w$방향으로 얼만큼 변했는지 나타냄



### 2.5.  Derivatives

- 도함수는 어떤 함수의 기울기를 뜻함. $x$ 가 조금 변할 때 $y$는 얼마나 변하는가? 
- $ \frac {d f(a)}{da}$ : $a$ 가 조금 변했을 때, $f(a)$가 얼마나 증가하는가?

- $f(a) = 3a$ 일 때, $ \frac {d f(a)}{da}=3$ . $a$ 값에 따라 기울기가 변하지 않음
- $f(a) = a^2$ 일 때, $ \frac {d f(a)}{da} = 2a$ . $a$ 값에 따라 기울기가 $2a$만큼 변함
- $f(a) = a^3$ 일 때, $ \frac {d f(a)}{da} = 3a^2$
- $f(a) = log_e^{(a)}$ 일 때, $ \frac {d f(a)}{da} = \frac{1}{a}$



### 2.6. **Computation Graph** 

- 미분에는 연쇄법칙이라는 것이 있음. 입력변수 $a$가 $a \rightarrow v \rightarrow J$ 순으로 영향을 미치는 함수가 있다면, $a$가 조금 변했을 때 $J$의 변화량은 어떠한가? 이는 $a$를 밀었을 때 $v$ 의 변화량과 $v$ 를 밀었을 때 $J$의 변화량의 곱이 됨. 즉,  $ \frac{dJ}{da}= \frac{dJ}{dv} \frac{dv}{da}$ 임. 이게 미분의 연쇄법칙.
- 예를 들어, $J = 3(a+bc)$ 라는 함수가 있다고 하자. $a=5, b=3, c=2$이다. 이때 $\frac{dJ}{da}$를 구한다고 하면, 먼저 $a+bc = v$라 보고, $\frac{dJ}{dv}$를 구한다. $J = 3v$이므로 $\frac{dJ}{dv} = 3$이다. 그 다음 $\frac{dv}{da}$ 를 구한다. $v = a + bc$이므로 $\frac {dv}{da} = 1$이다. 따라서 $\frac{dJ}{da} = \frac{dJ}{dv} \frac{dv}{da} = 3$  이 된다. 
- 한편, 위 $J$에서  $bc = u$  라고 하자. $\frac{dJ}{db} =\frac{dJ}{dv}\frac{dv}{du} \frac{du}{db} = \frac{dJ}{du} \frac{du}{db} = 3 \times 2 = 6$  
- 이렇게 $J$(final output variable)에 영향을 미치는 요소를 쪼개서 거꾸로 넘어가며 역방향으로 최초의 입력값 $a$이 $J$의 변화에 얼만큼 영향을 미치는지 알 수 있음 
- 참고로 코딩을 할 때는   $\frac{dJ}{da}$ 는 `da`,  $\frac{dJ}{dv}$ 는 `dv`라는 변수명을 사용할 것!



### 2.7. **Logistic Regression Gradient Descent** 

#### one sample case

- 로지스틱 회귀를 계산그래프로 표현해서 경사하강법으로 계산을 해보자
- 입력값으로부터 손실함수까지 이어지는 과정은 $ z = w_1x_1 + w_2x_2 + b \rightarrow a = \sigma(z) \rightarrow L(a,y)$ 로 표현할 수 있음. 거꾸로 하나씩 도함수를 계산해보면?
- 먼저, $a$ 가 변할 때 $L$의 변화는 $\frac {dL}{da} = -\frac {y}{a} + \frac{1-y}{1-a}$임.
- 다음으로, $z$ 가 변할 때 $L$의 변화는 $\frac {dL}{dz} = \frac{dL}{da}\frac {da}{dz} = (-\frac {y}{a} + \frac{1-y}{1-a})(a(a-1))$임
- 최종적으로 $w_1$이 변할 때 $L$의 변화는 $\frac {dL}{dw_1} = w_1 \frac{dL}{dz}$  
- 이렇게 계산해보고, $w_1 := w_1 - \alpha \frac{dL}{dw_1}$로 $w_1$을 갱신함
- 언제까지 갱신함? $ \frac{dL}{dw_1}$가 $0$이 될 때까지(=$L$을 더이상 낮출 수 없을 때까지) 갱신한다!



#### m samples case

- m개의 샘플에 대해 전체적으로 비용함수의 값을 최소화 하는 $w$를 찾으려면? 비용함수 $J(w,b) = \dfrac{1}{m} \sum\limits_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)})$이다. 
- m 샘플의 비용함수 $J$를 한 번 구해보면 아래와 같음

```
J = 0
dw1 = 0
dw2 = 0
db = 0
for i to m: 
	z_i = (x_T)(x_i) + b
	a_i = sigma(z_i)
	J += -[(y_i)(log(a_i))+ (1-y_i)log(1-a_i)] /*업데이트*/
	dw1 += (x1_i)(dz_i)
	dw2 += (x2_i)(dz_i)
	db += (dz_i)
	
J /= m
dw1 /= m
dw2 /= m
db /= m
```

- 이렇게 한 번 하고나면 , $\frac{dL}{dw_1}$ (= `dw1`) 가 나옴. 그럼 $w_1 := w_1 - \alpha \frac{dL}{dw_1}$로 $w_1$을 갱신하여 위 과정을 다시 반복함.