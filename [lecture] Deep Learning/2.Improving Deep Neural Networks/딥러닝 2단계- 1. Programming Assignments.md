# 딥러닝 2단계: 심층 신경망 성능 향상시키기

### 1. Initialization 

- 초기화를 잘 하면? 

  - 최적화 속도 높일 수 있음
  - 트레이닝 에러를 낮추는 방향으로 최적화 할 확률이 높아짐

- **초기화 방법 1. Zero initialization**

  - 각 층의 $W$와 $b$의 shape에 맞는 0으로 구성된 행렬을 초기 값으로 사용

    - 2층 신경망에서 각 층의 유닛 수를 $n^{[l]}$이라고 하면, 
      $W^{[l]}$의 `shape`은 $(n^{[l-1]}, n^{[l]})$이고, $b^{[l]}$ 의 `shape`은 $(n^{[l]}, 1)$이

    ```python
    def initialize_parameters_zeros(layers_dims):
      parameters = {}
      L = len(layers_dims)            # number of layers in the network
    
      for l in range(1, L):
          ### START CODE HERE ### (≈ 2 lines of code)
          parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1]))
          parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
          ### END CODE HERE ###
      return parameters
    ```

  - 예를 들어, 0층(입력층)의 데이터가 3개, 1층 신경망의 유닛이 2개, 2층 신경망의 유닛이 1개 라고 하면 `layers_dims = [3,2,1]`이다. 

    - `W1.shape = (layers_dim[1], layers_dim[0]) = (2,3)`이 되고
    - `b1.shape = (layers_dim[1], 1) = (2,1)`이 된다. 

    







### 3. Gradient Checking

> 딥러닝 모델을 이용해 사기거래 감지(fraud detection) 모델을 만들어보자. 이 문제는 특히나 mission critical한 문제이니 역전파 적용이 제대로 되었는지 확인을 해야한다! 어떻게 확인할 수 있을까? 
>
> **Gradient checking**을 쓰면 된다! 



먼저, $J = f(x) = \theta x$ 인 함수가 있다고 하자. 순전파 함수를 짜보면 다음과 같을 것이다. 

```python
def forward_propation(x, theta):
  J = theta * x
  return J
```

이 식의 역전파식을 만들어보자. $\frac{\delta J}{\delta \theta} = x$ 일 것이다. $\frac{\delta J}{\delta \theta}$ 는 간단하게 `dtheta` 라고 쓰자.

```python
def backward_propagation(x, theta):
  dtheta = x
  return dtheta
```

`backward_propagation` 값이 제대로 구현되었는지 보려면 **gradient checking** 을  해보자!

