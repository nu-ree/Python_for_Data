# 딥러닝 2단계 - 2. Optimization algorithms

## 1. Mini-batch gradient descent

- 딥러닝은 큰 데이터셋에서 더욱 효과를 발휘하는데, 데이터셋이 커질수록 학습 속도는 느려진다는 문제가 있음. 빠른 최적화 알고리즘을 선택해야 효율성을 높일 수 있음. 이걸 이번 주차에는 배워보자!
- 벡터화(vectorization)는 m개의 샘플에 대한 훈련 속도를 높여주긴 하지만, m이 아주 커지면 한 번에 전체를 훈련(**batch gradient descent**)시킨다는 것이 쉽지 않음
  - 트레이닝할 때 데이터셋을 나누어서 해보면 어떨까? **mini-batch gradient descent**
  - $X = [x^{(1)}, ..., x^{(5000000)}]$가 있다고 하자. m개의 샘플데이터가 있다. $X$의 shape은 $(n_x,m)$이다.
  - 이를 1000개씩의 mini-batch로 나누면?
  - $X = [X^{{1}}, ... , X^{{5000}}]$가 되고 $X^{{1}}$의 shape은 $(n_x, 1000)$이 될 것 
  - batch training에서는 트레이닝셋 전체가 1번 돌면, 기울기 강하가 1번 일어나는 반면, mini batch에서는 트레이닝 셋 전체가 한 번 돌면(=미니 배치 5000개가 돌면) 기울기 강하가 5000번 일어남. 여기서 트레이닝 셋이 한 번 도는 걸 **1 epoch**라고 함. 



## 2. Understanding mini-batch gradient descent

- 배치를 쓰건, 미니배치를 쓰건 epoch 수가 늘어남에 따라 비용함수 $J$는 감소하는 형태를 띄는데, 미니배치는 iter 횟수의 증가에 따른 비용함수의 그래프에 배치보다 노이즈가 좀 더 있을 수 있음. 배치마다 레이블이 잘못된 데이터가 많이 들어있을 수도 있고 해서 그런것. 하지만 전체적으로 epoch 횟수가 늘어날수록 비용함수가 감소한다는 점은 다르지 않음
- 미니배치의 사이즈를 잘 정해야 하는데 1부터 m사이의 적절한 수준을 선택해야 함
  - 미니 배치 사이즈  == 1 : **stochastic gradient descent**. 기울기 강하 과정이 매우 nosiy할 것. 한 번에 매우 작은 스텝을 이동할 것이고, 전체적으로 한 방향성을 갖고 움직이지도 않을 것. 가장 큰 문제는 백터화를 통한 속도 높이기가 불가능 하다는 것. 한 아이템씩 순차적으로 돌려야하니까. 
  - 미니 배치 사이즈 == m : **batch gradient descent**. 기울기 강하 과정이 큰 스텝으로 일어나고 한 방향성을 가지고 기울기 강하가 일어날 것. 다만 한 iteration당 소요 시간이 긴 단점. 
- 데이터 셋이 작다면(m <= 2000), 그냥 batch gradient descent를 쓰자
- 데이터셋이 좀 크다면 미니배치 사이즈를 64~512 정도로 설정(2의 n승 값을 쓰는 것이 좋음!)
- 메모리에 적절한 크기를 찾아야 함
- $2^{6}, 2^{7}, 2^{8}, 2^{9}$ 중 하나씩 시도해보면서 적절한 사이즈 결정함



## 3. Exponentially weighted averages

- 기울기 강하보다 빠른 최적화 알고리즘이 있는데, 이를 알기 위해서는 지수적 가중 평균(exponentially weighted averages, 지수적 이동 가중평균이라고도 함)이라는 개념을 알고 있어야 함

- 1월1일부터 12월 31일까지 날씨데이터($\theta_{1}, ... , \theta_{365}$)가 있다고 하자. 일자별 온도의 가중평균값을 구하면? 

  $ V_{0} = 0 $

  $ V_{1} = 0.9 V_{0} + 0.1 \theta_{1}$

  $ V_{2} = 0.9 V_{1} + 0.1 \theta_{2}$

  ... 

  $ V_{t} = \beta V_{t-1} + (1-\beta) \theta_{t}$라고 하면, $V_{t}$는 $\frac {1}{1-\beta}$ 간의 기온을 평균한 것. 

-  $\beta = 0.9$라면 $\frac{1}{1-0.9} = 10$일 간의 온도를 평균한 이 되는 것
-  $\beta = 0.98$라면 $\frac{1}{1-0.98} = 50$일 간의 온도를 평균한 이 되는 것

- $\beta$값이 커질수록 평균 값을 더 많은 일수로 구함 
  - 즉 이전 값에 더 큰 비중을 주고, 지금 값(최신값)에는 더 작은 비중을 주는 것. 
  - 평균 그래프의 커브가 훨씬 smooth해지고 실제 데이터 그래프보다 약간 우측으로 이동하게 될 것. 

- $\beta = 0.5$라면 $\frac{1}{1-0.5} = 2$일 간의 온도를 평균한 이 되는 것.
  - 기온 변화를 훨씬 빠르게 평균에 반영
  - $\beta$값이 클 때보다 더 평균 그래프가 노이지 해짐

## 4. Understanding exponentially weighted averages

- $V_{t} = \beta V_{t-1} + (1-\beta)\theta_{t}$ 로 지수적 가중평균을 구한다

  - $\beta = 0.9$라면, $V_{100} = 0.1\theta_{100} + 0.1\times0.9 \theta_{99} + 0.1\times(0.9)^{2} \theta_{98} + ... $ 
  - 과거 몇일간의 $\theta$ 값을 지수적으로 감소하는 함수에 곱한다고 생각하면 된다!

- 지수적 가중평균은 직전의 $V$ 값만 기억하면 되므로 메모리를 효율적으로 쓸 수 있는 연산 방법. 코드도 매우 간단

  ```python
  v = 0
  theta = [theta_1, theta_2, ... , theta_t]
  for i in range(t):
   v = beta*v + (1-beta)*theta[i] 
  ```

  

## 5. Bias correction in exponentially weighted averages

- $V_0 = 0$으로 시작하여 지수적 가중평균값을 구하게 되면, 초반에는 좋은 추정치를 얻기가 힘듦

- 어떻게 좀 더 적절한 값으로 보정할 수 있을까?
  - $V_t$대신에 $\frac{V_t}{1-\beta^{t}}$를 사용하면 보정할 수 있음
  - $\beta = 0.98, t = 2$이면 $1-\beta ^{t} = 1-(0.98)^2 = 0.0396 $ 임
    - $\frac{V2} {0.0396} = \frac{0.98\times0.002 \theta_1 + 0.02\theta_2}{0.0396}$로 계산되어 $\theta_1, \theta_2$ 의 가중평균값이 되어 바이어스가 제거 되었음!
    - $t$가 커질수록 $\frac{1}{1-\beta^t}$ 값은 매우 작아질 것이므로 뒷쪽 데이터에는 바이어스 보정의 효과 미미해짐! 굿~



## 6. Gradient descent with momentum

- 모멘텀은 일반 그래디언트 디센트보다 훨씬 빠름
- 기울기의 기하급수적 가중평균치를 산출 => 이 기울기를 이용해 weight를 업데이트!

- ????





## 7. RMSprop

- RMSprop(root mean square propagation)을 이용해서도 기울기 강하 속도를 높일 수 있음!
- ?????





## 8. Adam optimization algorithm

- Adam은 다양한 딥러닝 구조에서 잘 작동하는 옵티마이제이션 알고리즘(드문 일!)